---
title: "Modeling"
format: html
editor: 
  markdown: 
    wrap: 72
author: Powell Braddock
---

# Modeling

## Introduction

This page is a continuation of the EDA page for the project addressing
the creation and selection of a predictive model on the [Diabetes Health
Indicators
Dataset](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/)
from Kaggle.

The target (or response) variable is `Diabetes_binary`, where there are
two classes: **No Diabetes** and **Prediabetes or Diabetes**. We are
using all 21 predictor variables:
`HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, Genhlth, MentHlth, PhysHlth, DiffWalk, Sex, Age, Education,`
and `Income.` We are treating everything but `BMI` as a categorical
variable and `BMI` as a numeric variable.

In this page, we will explore and select a model that **predicts the
status of diabetes** in a patient the best.

We will use `tidymodels`, the log-loss metric, and 5 fold
cross-validation to select the best model from each of the two families
of models, then compare the two best models from each family.

## Data Split

Initally, we need to split the data into a training (70% of the data)
and test set (30% of the data). We've set a seed to make things
reproducible.

```{r}
diabetes_convert <- readRDS('diabetes_convert.rds')
library(tidymodels)
set.seed(11)

diabetes_convert <- diabetes_convert |>
  mutate(Diabetes_binary = factor(Diabetes_binary))
diabetes_split <- initial_split(diabetes_convert, prop = 0.7)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)

```

## Cross Validation Folds

We are using our split data to create our 5 cross validation folds.

```{r}

diabetes_CV_folds <- vfold_cv(diabetes_train, 5, strata = Diabetes_binary)

log_loss_metric <- metric_set(mn_log_loss)
```

Now we can create the `recipe` we will use for all of the models, where
`Diabetes_binary` is the response/outcome.

```{r}
diabetes_rec <-
  recipe(Diabetes_binary ~., data = diabetes_train) |>
  step_novel(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())
diabetes_rec
```

## Classification Tree

A classification tree model is a model that usually uses the most
prevalent class in a region as the prediction. The response is a
categorical variable, in our case: the `Diabetes_binary` variable. We
will split the predictor space up into region and makes the prediction
based on which space an observation ends up in. Classification trees are
nonlinear and are fairly easy to interpret!

Let's begin by defining the classification tree model using
`decision_tree()` and specify `cost_complexity` as a parameter to be
tuned, along with `rpart` as the engine.

```{r}
tree_mod <- 
  decision_tree(
    cost_complexity = tune()
  ) |>
  set_engine("rpart") |>
  set_mode("classification")
```

The next chunk of code is the creation of our workflow, where we add the
recipe and the model.

```{r}
tree_wkf <- workflow() |>
  add_recipe(diabetes_rec) |>
  add_model(tree_mod)
```

Now we can create our tuning grid that will help us find the optimal
`cost_complexity` we will tune for.

```{r}
tree_grid <- tibble(
  cost_complexity = c(0.0001, 0.001, 0.01, 0.1)
)
```

Let's tune! You can see we've included our workflow, resampling on the
Cross Validation Folds, tuning grid, and the log-loss metric.

```{r}
tree_tune <- tune_grid(
  tree_wkf,
  resamples = diabetes_CV_folds,
  grid = tree_grid,
  metrics = log_loss_metric,
  control = control_grid(save_pred = TRUE)
)
tree_tune
```

Now let's see how each of those models created performed when predicting
the training set.

```{r}
tree_tune |>
  collect_metrics()
```

Looks to me like the second model, with a `cost_complexity` of 0.001
performs the best. We can call it the `best_tree`.

```{r}
best_tree <- select_best(tree_tune, metric = 'mn_log_loss')
best_tree
```

Let's finalize that tree's fit into the object `tree_final_fit` we will
use later.

```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(best_tree)
tree_final_fit <- tree_final_wkf |>
  last_fit(diabetes_split, metrics = log_loss_metric)
```

## Random Forest

A Random Forest Model is an ensemble tree method where multiple trees
are created from bootstrap samples. These average results are what are
used to make the final prediction, and also help the overall model to
not become 'overfit'. Random Forests are less prone to drastic changes
in prediction with small changes in inputs than the classification tree
models. Let's see if we find that to be true for this data!

Let's begin by defining the random forest model using `rand_forest()`
and specify `mtry` as a parameter to be tuned, along with `ranger` as
the engine. I also like the look and feel of the importance plot from
the previous homework, so I included the importance specification as
'impurity'.

I've also created the workflow, which uses the same recipe as before,
but the random forest model this time.

```{r}
rf_mod <- rand_forest(mtry=tune(), trees=50) |>
  set_engine('ranger', importance = 'impurity') |>
  set_mode('classification')
rf_wkf <- workflow() |>
  add_recipe(diabetes_rec) |>
  add_model(rf_mod)
```

Let's fit the random forest models using the 5 fold cross-validation
like before and the log-loss metric.

```{r}
rf_fit <- rf_wkf |>
  tune_grid(resamples = diabetes_CV_folds,
            grid = 7,
            metrics = log_loss_metric)
```

Now we can compare the log-loss across the family of random forest
models to choose the 'best'.

```{r}
rf_fit |> collect_metrics()
```

Looks like our 'best' model is the one with an mtry value of 4!

```{r}
best_forest <- select_best(rf_fit, metric = 'mn_log_loss')
best_forest
```

We can finalize that model's fit to use and compare with the
classification tree 'best' model.

```{r}
rf_final_wkf <- rf_wkf |>
  finalize_workflow(best_forest)
rf_final_fit <- rf_final_wkf |>
  last_fit(diabetes_split, metrics = log_loss_metric)
```

## Final Model Selection

We now have two best models: Classification Tree and Random Forest.
Let's compare both models on the test set and declare an overall winner!

```{r}
rbind(
  tree_final_fit |> 
    collect_metrics() |>
    mutate(Model = 'Classification Tree', .before = '.metric'),
  rf_final_fit |> 
    collect_metrics()|>
    mutate(Model = 'Random Forest', .before = '.metric'))
```

Close call! It appears that the Random Forest Model was (slightly)
better at predicting on the test set... phew! That aligns with the idea
that the classification tree model might have overfit on the data and
the random forest model is more accurate due to the averaging.

We can extract that fit from our 'best overall' model!

```{r}
rf_final_fit |>
  extract_fit_parsnip()
```

Now let's use the script from homework 9 to create the importance plot.

```{r}
get_rand_for_importance <- function(x) {
  x |> 
    extract_fit_parsnip() |>
    vip::vi()
}

ctrl_imp <- control_grid(extract = get_rand_for_importance)

cells_resampling <-
  rf_final_wkf |>
  fit_resamples(diabetes_CV_folds, control = ctrl_imp)
cells_resampling

cells_resampling_2 <- cells_resampling |>
  select(id, .extracts) |>
  unnest(.extracts) |>
  unnest(.extracts) 

cells_resampling_2
ggplot(cells_resampling_2, aes(x=Variable, y=Importance)) +
  geom_bar(stat = 'identity')+
  labs(title='Variable Importance for Random Forest Model', x='Variable', y='Importance') +
  coord_flip()
```

BMI, as I secretly thought, is the MOST important variable for
predicting the status of diabetes. This super important variable is
followed closely by Age and General Health. Physical Health, Income, and
High Blood Pressure are the fourth, fifth, and sixth most importance for
this model.

Check out the API next to see how your attributes round up to predict
your diabetes status....

```{r}
saveRDS(rf_final_fit, file = 'rf_final_fit.rds')
```
